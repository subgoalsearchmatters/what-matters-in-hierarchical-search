import copy
import os

import numpy as np
from joblib import dump
from loguru import logger
from torch import Tensor

from carl.environment.env import GameEnv
from carl.environment.n_puzzle.tokenizer import NPuzzleTokenizer
from carl.environment.tokenizer import GameTokenizer
from carl.environment.utilis import HashableState


class NPuzzleCore:
    """
    This class is used to implement the core logic of the n-puzzle game.
    Moves are represented as integers:
    0: Left
    1: Right
    2: Up
    3: Down
    """

    def __init__(self, size_of_board: tuple[int, int] = (5, 5)) -> None:
        self._size_of_board = size_of_board
        assert self._size_of_board[0] == self._size_of_board[1], 'Board must be square'
        self._goal_state: np.ndarray = np.array(list(range(1, self._size_of_board[0] ** 2)) + [0])

    @property
    def size_of_board(self) -> tuple[int, int]:
        return self._size_of_board

    def available_actions(self, state: np.ndarray) -> list[int]:
        """
        Returns a list of available actions for a given state.
        """

        empty_space: int = int(np.where(state == 0)[0][0])
        n: int = self._size_of_board[0]
        moves: list[int] = [0, 1, 2, 3]

        if empty_space % n == 0:
            moves.remove(0)
        if empty_space % n == n - 1:
            moves.remove(1)
        if empty_space - n < 0:
            moves.remove(2)
        if empty_space + n > n * n - 1:
            moves.remove(3)

        return moves

    def next_step(self, state: np.ndarray, action: int) -> np.ndarray:
        """
        Returns the next state given a state and an action. If the action is not valid, the state is returned unchanged.
        """

        assert action in [0, 1, 2, 3], f'Invalid action: {action}, must be in [0, 1, 2, 3]'

        next_state: np.ndarray = copy.deepcopy(state)
        empty_space: int = int(np.where(state == 0)[0][0])
        n: int = self._size_of_board[0]

        valid_actions: list[int] = self.available_actions(state)

        if action in valid_actions:

            if action == 0:
                next_state[empty_space], next_state[empty_space - 1] = (
                    next_state[empty_space - 1],
                    next_state[empty_space],
                )
            elif action == 1:
                next_state[empty_space], next_state[empty_space + 1] = (
                    next_state[empty_space + 1],
                    next_state[empty_space],
                )
            elif action == 2:
                next_state[empty_space], next_state[empty_space - n] = (
                    next_state[empty_space - n],
                    next_state[empty_space],
                )
            else:
                next_state[empty_space], next_state[empty_space + n] = (
                    next_state[empty_space + n],
                    next_state[empty_space],
                )

        return next_state

    def next_unique_step(self, state: np.ndarray, action: int) -> np.ndarray:
        """
        Returns the next state given a state and an action. This method not allow for duplicate states.
        """

        assert action in self.available_actions(state), 'Invalid action'

        next_state: np.ndarray = copy.deepcopy(state)
        empty_space: int = int(np.where(state == 0)[0][0])
        n: int = self._size_of_board[0]

        if action == 0:
            next_state[empty_space], next_state[empty_space - 1] = (
                next_state[empty_space - 1],
                next_state[empty_space],
            )
        elif action == 1:
            next_state[empty_space], next_state[empty_space + 1] = (
                next_state[empty_space + 1],
                next_state[empty_space],
            )
        elif action == 2:
            next_state[empty_space], next_state[empty_space - n] = (
                next_state[empty_space - n],
                next_state[empty_space],
            )
        else:
            next_state[empty_space], next_state[empty_space + n] = (
                next_state[empty_space + n],
                next_state[empty_space],
            )

        return next_state

    def is_solved(self, state: np.ndarray) -> bool:
        """
        Returns True if the given state is solved, False otherwise.
        """

        return np.array_equal(state, self._goal_state)

    def generate_random_state_with_solution(
        self, number_of_moves: int
    ) -> tuple[np.ndarray, list[int], list[np.ndarray]]:
        """
        Generates a random state and its solution. args: number_of_moves: number of moves to generate the state.
        returns: state: the generated state. solution: the solution to the generated state, i.e. the sequence of
        moves to reach the goal state. trajectory: the trajectory of the generated state, i.e. the sequence of states
        generated by the sequence of moves.

        """

        state: np.ndarray = self._goal_state
        solution: list[int] = []
        trajectory: list[np.ndarray] = [state]

        for _ in range(number_of_moves):
            action: int = np.random.choice(self.available_actions(state))
            state = self.next_unique_step(state, action)
            solution.append(action)
            trajectory.append(state)

        trajectory.reverse()

        return state, solution, trajectory

    def generate_random_unique_dataset_with_solution(
        self,
        n_samples: int,
        n_steps: int,
        path_to_save: str | None = None,
        save_after_each: int | None = None,
    ) -> dict[int, list[np.ndarray]]:
        """
        Generates a dataset of random states with their solutions. args: n_samples: number of samples to generate.
        n_steps: number of steps to generate each sample, i.e. the number of moves to reach the goal state.
        path_to_save: path to save the dataset. save_after_each: save the dataset after each sample set. returns:
        trajectories: a dictionary of trajectories, where the key is the index of the sample and the value is the
        trajectory of the sample.

        """

        states: set[HashableState] = set()
        trajectories: dict[int, list[np.ndarray]] = {}
        part: int = 0

        if path_to_save is not None and save_after_each is None:
            logger.info(
                f'Saving dataset to {path_to_save}. Saving after each sample set to the default value: 1.'
            )
            save_after_each = 1

        while len(states) < n_samples:
            state, _, trajectory = self.generate_random_state_with_solution(n_steps)
            state_hash: HashableState = HashableState(state, None, None)

            if state_hash not in states:
                states.add(state_hash)
                trajectories[len(states) - 1] = trajectory

            if path_to_save is not None and len(trajectories) == save_after_each:
                logger.info(f'Saving dataset to {path_to_save}.')
                dump(trajectories, os.path.join(path_to_save, f'n_puzzle_trajectories_{part}.pkl'))
                trajectories.clear()
                part += 1

        return trajectories


class NPuzzleEnv(GameEnv):
    name: str = 'n_puzzle'

    def __init__(self, tokenizer: NPuzzleTokenizer) -> None:
        self._tokenizer = tokenizer
        self.core = NPuzzleCore(size_of_board=tokenizer.size_of_board)
        self.internal_state: np.ndarray | None = None

    @property
    def tokenizer(self) -> GameTokenizer:
        return self._tokenizer

    def detect_action(self, board_before: np.ndarray, board_after: np.ndarray) -> int:
        """
        Detects the action that was taken to go from board_before to board_after.
        """

        empty_before: int = int(np.where(board_before == 0)[0][0])
        empty_after: int = int(np.where(board_after == 0)[0][0])
        n: int = self.core.size_of_board[0]

        if empty_before == empty_after + 1:
            return 0
        elif empty_before == empty_after - 1:
            return 1
        elif empty_before == empty_after + n:
            return 2
        else:
            return 3

    @staticmethod
    def distribution_to_action(distribution: Tensor) -> int:
        """
        Converts a distribution to an action.
        """

        return distribution.argmax().item()

    def step(self, action: int) -> tuple[np.ndarray, float, bool, dict]:
        """
        Performs an action and returns the next state, the reward, whether the game is over and some info.
        """

        self.internal_state: np.ndarray = self.next_state(self.internal_state, action)
        reward: float = 0.0
        done: bool = self.is_solved(self.internal_state)
        info: dict = {}

        return self.internal_state, reward, done, info

    def restore_full_state_from_np_array_version(self, state: np.ndarray) -> None:
        self.internal_state = state

    def next_state(self, state: np.ndarray, action: int) -> np.ndarray:
        return self.core.next_step(state, action)

    def get_state(self) -> np.ndarray:
        return self.internal_state

    def is_solved(self, board: np.ndarray) -> bool:
        return self.core.is_solved(board)

    def show_state(
        self, state: np.ndarray, title: str | None = None, file_name: str | None = None
    ) -> None:
        raise NotImplementedError

    def show_many_states(self, states: list[np.ndarray], titles: list[str]):
        pass

    def set_state(self, state: np.ndarray) -> None:
        raise NotImplementedError
